<!DOCTYPE html>
<html>

  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG"></script>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Стохастический анализ в задачах. Машинное обучение и задача классификации.</title>
  <meta name="description" content="С чего всё началось\(\def \X {\mathcal X}\def \Y {\mathcal Y}\def \F {\mathcal F}\def \R {\mathbb R}\def \E {\mathbb E}\def \P {\mathbb P}\def \N {\mathcal N...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="/2015/10/27/stochanalysis-machine-learning.html">
  
  <link rel="alternate" hreflang="ru" href="/2018/02/11/sketches.html" />
  <link rel="alternate" hreflang="ru" href="/2016/01/27/discr-genf-group.html" />
  <link rel="alternate" hreflang="ru" href="/2015/10/27/stochanalysis-machine-learning.html" />
  
  <link rel="alternate" hreflang="" href="/css/main.css" />
  <link rel="alternate" hreflang="" href="/sitemaps.xml" />
  <link rel="alternate" hreflang="en" href="/feed.xml" />
  <link rel="alternate" hreflang="en" href="/old_site/index_old.html" />
  <link rel="alternate" hreflang="ru" href="/old_site/index_old_ru.html" />
  <link rel="alternate" type="application/rss+xml" title="" href="">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/"></a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">

        <!--<a class="page-link" href=""></a>-->
        
      </div>

      <!--
      <div class="trigger">
        
        
          
          <a class="page-link" href="/archive_ru.html">Записи</a>
          
        
          
        
          
          <a class="page-link" href="/old_site/index_ru.html">CV</a>
          
        
      </div>
      -->
    </nav>

  </div>


<div class="wrapper" style="text-align: right; line-height: 2em">
  
   <a href="/2018/02/11/sketches.html" class="ru">ru</a>  <a href="/2016/01/27/discr-genf-group.html" class="ru">ru</a>  <a href="/2015/10/27/stochanalysis-machine-learning.html" class="ru">ru</a> 

  
   <a href="/css/main.css" class=""></a>  <a href="/sitemaps.xml" class=""></a>  <a href="/feed.xml" class="en">en</a>  <a href="/old_site/index_old.html" class="en">en</a>  <a href="/old_site/index_old_ru.html" class="ru">ru</a> 
</div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Стохастический анализ в задачах. Машинное обучение и задача классификации.</h1>
    <p class="post-meta"><time datetime="2015-10-27T00:00:00+01:00" itemprop="datePublished">Oct 27, 2015</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h2 id="с-чего-всё-началось">С чего всё началось</h2>
<p>\(
\def \X {\mathcal X}
\def \Y {\mathcal Y}
\def \F {\mathcal F}
\def \R {\mathbb R}
\def \E {\mathbb E}
\def \P {\mathbb P}
\def \N {\mathcal N}
\def \Ind {\mathrm{Ind}\;}
\def \ch {\mathrm{ch}\;}
\)
Для третьекурсников в этом году существует факультативный курс «Стохастический анализ в задачах».</p>

<ul>
  <li>В прошлом семестре было законспектировано несколько занятий и опубликовано несколько листочков. Материалы и литературу можно найти в <a href="https://drive.google.com/open?id=0B733JIZxEnkNZEp0QUFfLS1vWmc">гугл-диске</a>.</li>
  <li>Идейным вдохновителем является сборник задач кафедры МОУ. Доступны <a href="http://arxiv.org/abs/1508.03461">Первая часть</a> и <a href="https://github.com/mipt-mou/zadavalnik-2/blob/master/main_part_2.pdf">Вторая часть</a> сборника. Первая часть находится в процессе рецензирования и на пути к изданию, вторая часть пока в процессе редактирования.</li>
</ul>

<p>Материал, который здесь приведен, рассказывал Никита Животовский.</p>

<h2 id="постановка-задачи-классификации">Постановка задачи классификации</h2>

<p>Практическая задача: пусть есть некоторые объекты \( \X \), и классы \( \Y 
\). Для удобства будем считать, что классов всего два, \( \Y \in \{-1, 1\} 
\). Объекты – произвольной природы.</p>

<p>Рассмотрим множество <em>классификаторов</em>:
\[
    \mathcal F = \{ f \mid f \colon \X \to \Y \}
\]</p>

<p>Вероятностная модель: объекты приходят парами \( \X \times \Y \). Пусть 
пришло \( n \) объектов.</p>

<p>На этих парах \( (X_1, Y_1), (X_2, Y_2), \ldots, (X_n, Y_n) \) задана 
вероятностная мера. Таким образом, мы вводим некоторое распределение на \( 
\X \times \Y \), и будем считать, что все наблюдаемые пары получены 
независимо и согласно \( \P_{\X \times \Y} \).</p>

<p>Как же на самом деле взаимодействуют эти пары?</p>

<blockquote>
  <p>Предположение.</p>

  <p>\( \exists f^{\ast} \in \F \colon Y = f^{\ast} (X) \).</p>
</blockquote>

<p>Другими словами, мы предположили, что классы \( Y \) каким-то детерминированным 
образом зависят от \( X \), хотя она нам и неизвестна. Мы знаем, что эта 
функция лежит в классе \( \F \), и этот класс нам известен.</p>

<p>Не имеет значения, конечный он, счётный или несчётный. Он представляет собой 
некоторый «мешок», в котором лежат все «подозрительные» функции.</p>

<p>Наша задача стоит следующим образом: «приблизить» с помощью наблюдаемых 
данных функцию \( f^{\ast} \). Как можно это сделать?</p>

<p><strong>Вопрос.</strong> В какой метрике происходит приближение?</p>

<p><strong>Ответ.</strong> Наша задача – найти такую функцию \( \hat f(x) \) (построить её по 
данным), чтобы вероятность
    \[
        \P(\hat f(x) \neq f^{\ast} (x))
    \]
    была минимальна.</p>

<p>Давайте найдём процедуру, которая бы по данным строила такую оптимальную \( 
\hat f(x) \). Первое, что приходит в голову – минимизировать 
<em>эмпирический риск</em>: выбрать такую функцию \( \hat f \) по данным, что 
\[
    \sum_{i=1}^{n} \Ind [\hat f(X_i) \neq Y_i] \to \min
\]
Такой минимизатор риска не может ошибаться на выборке, потому что истинная 
зависимость лежит в классе, а минимум мы ищем по функциям класса. Другое дело в 
том, что может найтись довольно много таких различных классификаторов, которые 
согласуются на точках выборки.</p>

<p>Наглядный пример: рассмотрим синусоиду и константу \( 0 \), при том, что 
выборка нами наблюдается в нулях синусоиды. С точки зрения наблюдателя эти 
функции проецируют одни и те же наблюдения.</p>

<blockquote>
  <p>Определение.  Пусть \( X \) – случайная величина, \( \lambda \gt 0 \),
\( \varphi_X \colon \R_+ \to \R_+. \)</p>

  <p><em>Производящей функцией моментов</em> называется 
функция вида \[ \varphi_X (\lambda) = \E \exp(\lambda X) \]
Для каждого конкретного \( \lambda \) это матожидание является конкретным 
числом (для дискретной случайной величины это дискретное матожидание).</p>

  <p><em>Характеристическая функция</em> – это функция вида
\[ \chi_X(t) = \E \exp( i t X) \]</p>
</blockquote>

<p><strong>Вопрос.</strong> Пусть мы знаем, что эта функция является гладкой, дифференцируемой. Как 
найти 
математическое ожидание \( X \), зная эту функцию?</p>

<p><strong>Ответ.</strong>
\[
    \varphi’(0) = \E X \exp(0 X) = \E X %’
\]</p>

<p>Мы видим, что эта функция даёт довольно много информации о случайной величине, 
математическое ожидание, моменты.</p>

<blockquote>
  <p>Пример.</p>

  <p>Рассмотрим бернуллиевскую случайную величину, принимающую 
значения \( \pm 1 \) с вероятностями \( p = 1/2 \).
\[
  \varphi_X(\lambda) = \dfrac{1}{2} ( \exp(\lambda) + \exp(-\lambda) ) = 
      \ch(\lambda)
\]</p>
</blockquote>

<p>Оказывается, что для нормальной случайной величины выполнено свойство
\[
        \varphi_X(\lambda) \leqslant \exp \left(
            \dfrac{\lambda^2 \sigma^2}{2}
        \right)
\]
Заметим, что для экспоненты выполнено разложение в ряд:
\[
    \E \exp(\lambda X) = \E (1 + \lambda x + \dfrac{\lambda^2 x^2}{2} + 
    \ldots)
\]</p>

<blockquote>
  <p>Определение.</p>

  <p><em>Субгауссовской случайной величиной</em> будем называть такую случайную 
величину, матожидание которой равно нулю, и для которой найдётся \( \sigma 
\colon \)
\[
    \varphi_X(\lambda) \leqslant \exp \left(\dfrac{\lambda^2 \sigma^2}{2} 
    \right)
\]</p>
</blockquote>

<p>Некоторый физический смысл: мы хотим, чтобы моменты случайной величины не росли 
слишком быстро. Сейчас не нужно даже понимать, что такое гауссовская случайная 
величина, главное – это как ограничена производящая функция моментов 
субгауссовской случайной величины.</p>

<p>Пусть \( Y_1, \ldots, Y_n \) – субгауссовские величины с параметром \( \sigma 
\) (могут быть зависимы между собой). Посмотрим на максимум от этих случайных 
величин (это случайная величина).</p>

<p>Обозначим случайную величину \( Z \):
\[
    Z = \E \max_{i=1, \ldots, n} Y_i
\]</p>

<p>Мы хотим ограничить этот максимум сверху. Чуть позже мы поймём, что максимум 
будет тем больше, чем больше в этих случайных величинах «независимости».
\[
     \varphi_Z(\lambda) = \E \exp (\lambda Z) = \E \exp( \lambda \max_i Y_i)
     = \E \max_i \exp(\lambda Y_i)
\]</p>

<p><strong>Вопрос.</strong> Как максимум от неотрицательных случайных величин можно ограничить сверху 
каким-нибудь наивным способом?</p>

<p><strong>Ответ.</strong>
Максимум можно оценить суммой. В случае в экспонентами это не будет очень 
грубо.</p>

<p>\[
    \E \max_i \exp(\lambda Y_i) \leqslant \E \sum_{i} \exp(\lambda Y_i) = 
    \sum_i \E \exp(\lambda Y_i) = n \exp(\lambda^2 \sigma^2 / 2)
\]</p>

<blockquote>
  <p>Неравенство Йенсена.</p>

  <p>Пусть \( \psi \) – выпуклая функция.
\[
    \psi(\E X) \leqslant \E \psi (X)
\]</p>
</blockquote>

<p><strong>Вопрос.</strong>
Как с помощью этого неравенства, зная верхнюю оценку на производящую 
функцию, достать матожидание?
\[
    \varphi_Z(\lambda) \leqslant n \exp( \lambda^2 \sigma^2 / 2)
\]</p>

<p><strong>Ответ.</strong> Просто «в лоб» дифференцировать под знаком неравенства нельзя. Но у нас 
есть неравенство Йенсена, его и нужно использовать! Обратите также внимание на 
то, что хоть величины \( Y_i \) были центрированные, но максимум из них, то 
есть величина \( Z \) уже не является центрированной.
\[
    \E \exp(\lambda Z) \geqslant \exp( \lambda \E Z)
\]
\[
    \dfrac{1}{\lambda} \log \varphi_Z(\lambda) \geqslant \E Z
\]
\[
    \E \max_i Y_i \leqslant \dfrac{1}{\lambda} \ln \left(
        n \exp (\lambda^2 \sigma^2 / 2)
    \right) = \dfrac{1}{\lambda} \ln n + \dfrac{\lambda \sigma^2}{2}, \quad 
    \lambda \gt 0
\]
Так как это верно для любого \( \lambda \), давайте найдём минимум:
\[
    \lambda^\ast = \dfrac{\sqrt{2 \ln n}}{\sigma}, \qquad \E \max_i Y_i 
    \leqslant \sigma \sqrt{2 \ln n}.
\]
Пусть \( Z = \max_i |Y_i| \), где \( Y_i \) – центрированные 
субгауссовские величины. Заметим, что
\[
    Z = \max_i (Y_i, -Y_i), \qquad
    \E Z \leqslant \sigma \sqrt{2 \ln (2n)}.
\]</p>

<h2 id="снова-классификация">Снова классификация</h2>

<p>Классификатор, который минимизирует эмпирический риск, не ошибается на 
наблюдениях, где количество наблюдений-пар равно \( m \).
\[                                  <br />
    \P( \hat f(x) \neq f^\ast (x)) 
\]
В этом выражении есть некоторая тонкость: \( x \) – случайная величина, но 
при этом \( \hat f \) тоже строится по наблюдениям, то есть является случайной 
функцией. Вероятность берётся только по «новому наблюдению» \( x \), а затем 
можно усреднить эту величину по пространству вероятностей для классификатора:
\[
    \E^{(m)} \P ( \hat f(x) \neq f^{\ast} (x) ) = (1)
\]
Пусть \( \hat f \) не ошибается на обучающей выборке \( (X_1, Y_1), \ldots, 
(X_m, Y_m) \).
\[
    (1) = \E^{(m)} \left(
        \P\left( \hat f(X) \neq f^{\ast} (X) \right) - \dfrac{1}{m} 
        \sum_{i=1}^{m} \Ind 
        \left(\hat f(X_i) \neq Y_i\right)
    \right) = (2)
\]
Это выражение тождественно равняется \( (1) \), потому что второе слагамемое 
равно нулю.
\[
    (2) \leqslant \E^{(m)} \sup_{f \in \F} \left(
        \P \left(
            f(X) \neq f^{\ast} (X)
        \right) - \dfrac{1}{m} \sum_{i=1}^{m}
        \Ind \left(
            f(X_i) \neq Y_i
        \right)
    \right) = (3)
\]
Здесь второе слагаемое уже ненулевое по матожиданию. Заметим, что
\[
    \E \Ind \left(
        f(X_i) \neq Y_i
    \right) = \P \left(
        f(X_i) \neq Y_i
    \right)
\]
Можно из того же распределения \( \P \) сгерерировать новую выборку из 
\( m \) объектов. При этом в выражении \( (3) \) в первом слагаемом вероятности 
для \( \P(f(x) \neq f^{\ast}(x)) \) одинаковые для всех «искусственных» точек 
выборки. Это позволяет записать выражение в виде:
\[
    (3) = \E^{(m)} \sup_{f \in \F} \left(
        \E^{(m)} \left(
            \dfrac{1}{m} \sum \Ind (f(X’_i) \neq Y’_i)
        \right) - \dfrac{1}{m} \sum_i \Ind (f(X_i) \neq Y_i)
    \right) = (4)
\]
Воспользуемся неравенством Йенсена. Оказывается, что если класс 
<em>счётный</em>, то супремум \( \sup_{f \in \F} (\cdot) \) является выпуклой 
функцией (это известный и несложный факт из выпуклого анализа: надграфик от 
максимума является пересечением надграфиков выпуклых функций, каждый из которых 
является выпуклым множеством).
\[
    (4) \leqslant \E^{(m)} \E^{(m’)} \sup_{f \in \F}
    \left(
        \dfrac{1}{m} \sum_{i = 1}^{m} \Big(
            \Ind (f(X’_i) \neq Y’_i) - \Ind (f(X_i) \neq Y_i)
        \Big)
    \right) = (5)
\]
Рассмотрим отдельно разность индикаторов.
\[
    \Ind(f(X’_i) \neq Y_i) - \Ind (f(X_i) \neq Y_i)
\]
Эта случайная величина может принимать всего три значения: \( \{0, -1, 1\}. \) 
Между ними есть некоторое «равноправие»: пары \( (X_i, Y_i) \) распределены 
так же, как пары  \( (X’_i, Y’_i)  \). Значит, мы смотрим 
<em>разность</em> между одинаково распределёнными независимыми случайными 
величиными. Поэтому можно домножить выражение на независимый случайно 
распределённый знак \( \varepsilon_i \):
\[
    \varepsilon_i \Big(
        \Ind(f(X’_i) \neq Y_i) - \Ind (f(X_i) \neq Y_i)
    \Big).
\]
\[
    (5) = \E^{(m)} \E^{(m)} \E_{\varepsilon_i} \sup_{f \in \F}
    \left(
    \dfrac{1}{m} \sum_{i = 1}^{m} \varepsilon_i\Big(
    \Ind (f(X’_i) \neq Y’_i) - \Ind (f(X_i) \neq Y_i)
    \Big)
    \right) = (6)
\]
Сейчас мы упростим выражение, и этот крокодил превратится в очень простую 
штуку! Заметим, что \( \sup (a + b) \leqslant \sup (a) + \sup (b). \) Поэтому 
можно оценить выражение как
\[
    (6) \leqslant 2 \E^{(m)} \E_{\varepsilon} \sup_{f \in \F} \left(
        \dfrac{1}{m} \sum_{i=1}^{m} \varepsilon_i \Ind (f(X_i) \neq Y_i)
    \right)
\]
Практически уже сейчас можно применить нашу субгауссовскую технологию, главное 
– это понять, какие здесь нужно рассмотреть случайные величины. Зафиксируем 
\( m \) точек, по которым рассматривается матожидание, и рассмотрим выражение, 
которое стоит внутри скобки:
\[
    \E_{\varepsilon} \sup_{f \in \F} \left(
    \sum_{i=1}^{m} \varepsilon_i \Ind (f(X_i) \neq Y_i)
    \right)
\]
Заметим, что случайные величины, которые стоят под матожиданием, являются 
субгауссовскими (просто потому, что они принимают конечный набор значений, а 
точнее, \(\{0, \pm 1\}.\) Докажем это.</p>

<blockquote>
  <p>Утверждение.</p>

  <p>Случайная величина вида
\[
    \sum_{i=1}^{m} \varepsilon_i \Ind (f(X_i) \neq Y_i)
\]
является субгауссовской.</p>
</blockquote>

<h3 id="доказательство">Доказательство.</h3>

<p>\[
    \E \exp(\lambda \sum_{i=1}^{m} \varepsilon_i \Ind (f(X_i) \neq Y_i) ) = 
    (7)
\]
Экспонента от суммы является произведением экспонент, а матожидание 
произведения независимых случайных величин равняется произведению 
матожиданий. Кроме того, можно заметить, что \( \ch x \geqslant 1 \), 
поэтому можно «не учитывать» нулевые значения внутри экспоненты, если мы 
хотим оценить выражение сверху:
\[
    (7) = \Big(\E_{\varepsilon} \exp (\lambda \varepsilon_i \Ind (f(X_i) 
    \neq Y_i)) 
    \Big)^m = ( \ch \lambda )^{m}
\]
Заметим, что для гиперболического косинуса выполнено неравенство (оно 
доказывается разложением в бескеонечный ряд Тейлора):
\[
    \ch \lambda \leqslant \exp \left(
        \dfrac{\lambda^2}{2}
    \right)
\]
Значит,
\[
(7) \leqslant \exp \left(
    \dfrac{\lambda^2 m}{2}
\right)
\]
Таким образом, мы доказали, что наша величина является субгауссовской, с 
константой \( \sigma^2 = m \).</p>

<p><strong>Конец доказательства.</strong></p>

<p>Пусть класс функций является конечным: \( |\F| = N \). Мы знаем, что максимум 
из субгауссовских случайных величин ведёт себя следующим образом:
\[
    (6) \leqslant \dfrac{2 \E^{(m)} \sqrt{m} \sqrt{2 \ln (N)}}{m} = 
    2\sqrt{\dfrac{2 \ln N}{m}}.
\]
<strong>Таким образом, ожидаемая вероятность ошибки построенного нами алгоритма 
можно 
оценить сверху. С ростом числа точек это выражение будет стремиться к нулю.</strong></p>

<p><strong>Примечание.</strong> Оказывается, можно показать, что лучше, чем \( \sqrt{\ln 
N} \) в общем случае 
показать нельзя, но в нашей задаче результат можно улучшить. Можно доказать, 
что 
\[
    \E^{(m)} \P (\hat f(x) \neq f^{\ast} (x)) \leqslant \dfrac{4 \ln N}{m}.
\]
Эта оценка является «самой лучшей». Предположим, что существует некоторый 
«противник», который умеет варьировать \( N \) в зависимости от \( m \). Если 
словарь функций становится слишком большим, то техника рушится.</p>

<p>Пусть \( \F \) – пространство всех измеримых функций. Мы рассмотрим ситуацию, 
которую принять называть <em>переобучением</em>. Вероятность ошибки может 
оказаться очень большой, и даже стремиться к единице. Ясно, что так как мы 
наблюдаем выборку лишь в конечном числе точек, то найдётся функция, которая не 
ошибается на наблюдённых точках. Вывод: нельзя использовать слишком сложную 
идею, имея 
слишком мало данных!</p>

<h2 id="упражнения">Упражнения.</h2>

<ol>
  <li>Найдите характеристическую случайную функцию нормально 
распределённой 
случайной величины: \( X \sim \N(0, 1). \) \( \varphi_X(\lambda) = ? 
\)</li>
  <li><strong>Метод Чернова.</strong>
Используя неравенство Маркова, докажите, что для случайной величины \( X \) с производящей  функцией моментов \(\psi_{X}:\)
\[
 \P(X \ge \varepsilon) \le \inf\limits_{\lambda &gt; 0}\exp(\psi_{X}(\lambda) - \lambda\varepsilon) 
\]
<em>Подсказка:</em>
Функция \( g \), заданная \( g(x) = \exp(\lambda x) \) является возрастающей для \( \lambda \gt 0\).</li>
  <li>С помощью метода Чернова оцените \( P(X \ge \varepsilon) \), где \( X \sim \mathcal{N}(0, 1).\)</li>
  <li><strong>Неравенство Чернова.</strong>
Пусть \( S \sim \mathrm{Bi}(n, p)\). Докажите, что
\[                         <br />
 \P \left(\frac{S - \E S}{n} \ge \varepsilon\right) \le \exp(-2\varepsilon^{2}n).
\]
<em>Подсказка:</em>
Как задается производящая функция моментов для суммы независимых случайных величин?</li>
  <li>Докажите, что для неотрицательной случайной величины \( X \):
\[
\E X \le \int\limits_{0}^{\infty} \P(X \ge \varepsilon) d\varepsilon.
\]
6.
Пусть для \( c_{1} \gt e^{-1}, c_{2} \gt 0 \) для некоторой неотрицательной случайной величины \( X_n \) выполнено:
\[
\P(X_n \ge \varepsilon) \le c_{1}\exp(-c_{2}n\varepsilon^{2}).
\]
Докажите, что
\[
\E X_n \le \sqrt{\frac{C}{n}}, 
\]
где \( C = (1 + \ln(c_{1}))/c_{2}. \)</li>
</ol>



    <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'electrictric-github';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading"></h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li></li>
          <li><a href="mailto:"></a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/electric-tric"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">electric-tric</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
